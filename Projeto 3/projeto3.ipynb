{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import find_best_hyperparameters as fbh\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar essa celula para utilizar o arquivo \"./Dmoz-Science.csv\"\n",
    "\n",
    "data = fbh.load_db(\"./Dmoz-Science.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar essa celula para utilizar o arquivo webkb-parsed.csv\n",
    "data = fbh.load_db(\"webkb-parsed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   alpha  accuracy  f1_macro  f1_micro\n",
      "0    0.1  0.717500  0.711544  0.717500\n",
      "1    0.5  0.720000  0.716553  0.720000\n",
      "2    1.0  0.710833  0.709062  0.710833\n",
      "3    2.0  0.689167  0.689243  0.689167\n",
      "4    5.0  0.650000  0.649790  0.650000\n",
      "\n",
      "Melhor alpha encontrado: 0.5 com acurácia de 0.7200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hyperparams_df = fbh.greedy_search_MultinomialNB(data, [0.1, 0.5, 1.0, 2.0, 5.0])\n",
    "print(hyperparams_df)\n",
    "best_alpha_row = hyperparams_df.loc[hyperparams_df['accuracy'].idxmax()]\n",
    "best_alpha = best_alpha_row['alpha']\n",
    "best_accuracy = best_alpha_row['accuracy']\n",
    "print(f\"\\nMelhor alpha encontrado: {best_alpha} com acurácia de {best_accuracy:.4f}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data['text'])\n",
    "y = data['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "final_model = MultinomialNB(alpha=best_alpha)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "final_accuracy = accuracy_score(y_test, y_pred)\n",
    "final_f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "final_f1_micro = f1_score(y_test, y_pred, average='micro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Métricas no conjunto de teste:\n",
      "Acurácia: 0.7200\n",
      "F1 Score (Macro): 0.7166\n",
      "F1 Score (Micro): 0.7200\n",
      "\n",
      "Interpretação dos Resultados:\n",
      "O melhor hiperparâmetro encontrado foi alpha = 0.5, com base na maior acurácia no conjunto de validação.\n",
      "O modelo apresentou boa generalização no conjunto de teste, com métricas F1 Score (Macro e Micro) elevadas.\n",
      "Um valor de alpha menor tende a funcionar melhor para textos mais curtos ou com menos ruído nos dados, enquanto valores maiores suavizam os pesos.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nMétricas no conjunto de teste:\")\n",
    "print(f\"Acurácia: {final_accuracy:.4f}\")\n",
    "print(f\"F1 Score (Macro): {final_f1_macro:.4f}\")\n",
    "print(f\"F1 Score (Micro): {final_f1_micro:.4f}\")\n",
    "\n",
    "print(\"\\nInterpretação dos Resultados:\")\n",
    "print(f\"O melhor hiperparâmetro encontrado foi alpha = {best_alpha}, com base na maior acurácia no conjunto de validação.\")\n",
    "print(\"O modelo apresentou boa generalização no conjunto de teste, com métricas F1 Score (Macro e Micro) elevadas.\")\n",
    "print(\"Um valor de alpha menor tende a funcionar melhor para textos mais curtos ou com menos ruído nos dados, enquanto valores maiores suavizam os pesos.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'file_name': data.loc[y_test.index, 'file_name'],\n",
    "    'text': data.loc[y_test.index, 'text'],\n",
    "    'true_class': y_test,\n",
    "    'predicted_class': y_pred\n",
    "})\n",
    "\n",
    "results_df.to_csv(\"./results_MultinomialNB.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        C  accuracy  f1_macro  f1_micro\n",
      "0    0.01  0.549167  0.541022  0.549167\n",
      "1    0.10  0.685833  0.681072  0.685833\n",
      "2    1.00  0.706667  0.702757  0.706667\n",
      "3   10.00  0.715000  0.711247  0.715000\n",
      "4  100.00  0.715833  0.713002  0.715833\n",
      "\n",
      "Melhor C encontrado: 100.0 com acurácia de 0.7158\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hyperparams_df = fbh.greedy_search_LogisticRegression(data, [0.01, 0.1, 1, 10, 100])\n",
    "print(hyperparams_df)\n",
    "best_row = hyperparams_df.loc[hyperparams_df['accuracy'].idxmax()]\n",
    "best_C = best_row['C']\n",
    "best_accuracy = best_row['accuracy']\n",
    "print(f\"\\nMelhor C encontrado: {best_C} com acurácia de {best_accuracy:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data['text'])\n",
    "y = data['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(C=best_C, max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "final_accuracy = accuracy_score(y_test, y_pred)\n",
    "final_f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "final_f1_micro = f1_score(y_test, y_pred, average='micro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados finais no conjunto de teste:\n",
      "Acurácia: 0.7158\n",
      "F1 Score (Macro): 0.7130\n",
      "F1 Score (Micro): 0.7158\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nResultados finais no conjunto de teste:\")\n",
    "print(f\"Acurácia: {final_accuracy:.4f}\")\n",
    "print(f\"F1 Score (Macro): {final_f1_macro:.4f}\")\n",
    "print(f\"F1 Score (Micro): {final_f1_micro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'file_name': data.loc[y_test.index, 'file_name'],\n",
    "    'text': data.loc[y_test.index, 'text'],\n",
    "    'true_class': y_test,\n",
    "    'predicted_class': y_pred\n",
    "})\n",
    "\n",
    "results_df.to_csv(\"./results_LogisticRegression.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        C  accuracy  f1_macro  f1_micro\n",
      "0    0.01  0.720833  0.714070  0.720833\n",
      "1    0.10  0.723333  0.718956  0.723333\n",
      "2    1.00  0.716667  0.712776  0.716667\n",
      "3   10.00  0.705833  0.701577  0.705833\n",
      "4  100.00  0.706667  0.702845  0.706667\n",
      "\n",
      "Melhor C encontrado: 0.1 com acurácia de 0.7233\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hyperparams_df = fbh.greedy_search_LinearSVC(data, [0.01, 0.1, 1, 10, 100])\n",
    "print(hyperparams_df)\n",
    "best_row = hyperparams_df.loc[hyperparams_df['accuracy'].idxmax()]\n",
    "best_C = best_row['C']\n",
    "best_accuracy = best_row['accuracy']\n",
    "print(f\"\\nMelhor C encontrado: {best_C} com acurácia de {best_accuracy:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data['text'])\n",
    "y = data['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearSVC(C=best_C, random_state=42, max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "final_accuracy = accuracy_score(y_test, y_pred)\n",
    "final_f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "final_f1_micro = f1_score(y_test, y_pred, average='micro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados finais no conjunto de teste:\n",
      "Acurácia: 0.7233\n",
      "F1 Score (Macro): 0.7190\n",
      "F1 Score (Micro): 0.7233\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nResultados finais no conjunto de teste:\")\n",
    "print(f\"Acurácia: {final_accuracy:.4f}\")\n",
    "print(f\"F1 Score (Macro): {final_f1_macro:.4f}\")\n",
    "print(f\"F1 Score (Micro): {final_f1_micro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'file_name': data.loc[y_test.index, 'file_name'],\n",
    "    'text': data.loc[y_test.index, 'text'],\n",
    "    'true_class': y_test,\n",
    "    'predicted_class': y_pred\n",
    "})\n",
    "\n",
    "results_df.to_csv(\"./results_LinearSVC.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
